{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Day 3\n",
    "\n",
    "### Expert Question Answerer for InsureLLM\n",
    "\n",
    "LangChain 1.0 implementation of a RAG pipeline.\n",
    "\n",
    "Using the VectorStore we created last time (with HuggingFace `all-MiniLM-L6-v2`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched langchain.verbose, langchain.debug, langchain.llm_cache\n"
     ]
    }
   ],
   "source": [
    "# Fix ALL missing attributes from LangChain 0.3.x (ChatOpenAI + retrievers need these)\n",
    "import langchain\n",
    "\n",
    "# Add missing attributes if they don't exist\n",
    "if not hasattr(langchain, \"verbose\"):\n",
    "    langchain.verbose = False\n",
    "\n",
    "if not hasattr(langchain, \"debug\"):\n",
    "    langchain.debug = False\n",
    "\n",
    "if not hasattr(langchain, \"llm_cache\"):\n",
    "    langchain.llm_cache = None\n",
    "\n",
    "print(\"Patched langchain.verbose, langchain.debug, langchain.llm_cache\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL = \"moonshotai/kimi-k2-instruct-0905\"\n",
    "DB_NAME = \"vector_db\"\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Chroma; use Hugging Face all-MiniLM-L6-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "vectorstore = Chroma(persist_directory=DB_NAME, embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the 2 key LangChain objects: retriever and llm\n",
    "\n",
    "#### A sidebar on \"temperature\":\n",
    "- Controls how diverse the output is\n",
    "- A temperature of 0 means that the output should be predictable\n",
    "- Higher temperature for more variety in answers\n",
    "\n",
    "Some people describe temperature as being like 'creativity' but that's not quite right\n",
    "- It actually controls which tokens get selected during inference\n",
    "- temperature=0 means: always select the token with highest probability\n",
    "- temperature=1 usually means: a token with 10% probability should be picked 10% of the time\n",
    "\n",
    "Note: a temperature of 0 doesn't mean outputs will always be reproducible. You also need to set a random seed. We will do that in weeks 6-8. (Even then, it's not always reproducible.)\n",
    "\n",
    "Note 2: if you want creativity, use the System Prompt!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=MODEL,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 5}     # or any RETRIEVAL_K\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### These LangChain objects implement the method `invoke()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "\n",
    "# PATCH missing attributes due to LangChain 0.3.x bugs\n",
    "langchain.verbose = False\n",
    "langchain.debug = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are Ayush Tyagi's personal AI assistant. Use the retrieved context to answer.\"),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\")\n",
    "])\n",
    "\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "def rag_chain(question):\n",
    "    # retrieve documents\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    context = \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "    # run prompt + LLM\n",
    "    response = llm.invoke(prompt.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    ))\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain patches applied.\n"
     ]
    }
   ],
   "source": [
    "# CELL: Patch LangChain global variables (fixes debug + verbose error)\n",
    "import langchain\n",
    "\n",
    "if not hasattr(langchain, \"verbose\"):\n",
    "    langchain.verbose = False\n",
    "\n",
    "if not hasattr(langchain, \"debug\"):\n",
    "    langchain.debug = False\n",
    "\n",
    "print(\"LangChain patches applied.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Ayush Tyagi is a Computer Science Engineering student at JIMS, Greater Noida (IP University) set to graduate in 2025. He specializes in front-end development, creating visually stunning and user-friendly web applications. \\n\\nHis background includes:\\n- Education: PCM stream at Vivekanand School, Anand Vihar (2021) followed by B.Tech CSE at JIMS Greater Noida\\n- Technical skills: Front-end development, Python, Unity Engine, C#, and AI frameworks\\n- Professional experience: Game Development Intern at Tara Application, where he worked on mobile game reskinning, monetization, and publishing\\n- Notable projects: Built a multimodal AI assistant combining image generation and personalized chat capabilities\\n\\nHe's currently available for internships and collaborative projects, with a focus on blending creative design with technical execution to deliver exceptional digital experiences. You can reach him at tyagiayush239@gmail.com or through his LinkedIn profile.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 192, 'prompt_tokens': 902, 'total_tokens': 1094, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_name': 'moonshotai/kimi-k2-instruct-0905', 'system_fingerprint': None, 'id': '118e1ec30c584056ae59fb56a54f9e13', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--02d9d7a1-2be0-4aa6-bf28-a5ad497de89e-0' usage_metadata={'input_tokens': 902, 'output_tokens': 192, 'total_tokens': 1094, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain(\"Who is Ayush Tyagi?\")\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Vivekanand (formally ‚ÄúSwami Vivekanand‚Äù) was a 19th-century Indian monk, philosopher, and social reformer who inspired the name of Vivekanand School, Anand Vihar‚Äîthe school from which Ayush Tyagi completed his 12th-grade PCM education in 2021.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 64, 'prompt_tokens': 731, 'total_tokens': 795, 'completion_tokens_details': None, 'prompt_tokens_details': None, 'reasoning_tokens': 0}, 'model_name': 'moonshotai/kimi-k2-instruct-0905', 'system_fingerprint': None, 'id': '60177d1c4cf94777aac694a05a211883', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--ce89477c-0d9e-4681-8baa-bb5e90e0014d-0' usage_metadata={'input_tokens': 731, 'output_tokens': 64, 'total_tokens': 795, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "answer = rag_chain(\"Who is vivekanand?\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to put this together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT_TEMPLATE = \"\"\"\n",
    "You are Ayush Tyagi‚Äôs personal AI assistant. \n",
    "and give your intro when someone greets you.\n",
    "Your job is to answer questions using the provided context and your general knowledge.\n",
    "\n",
    "Follow this skill hierarchy when responding:\n",
    "1. Prioritize knowledge related to LLMs, vector databases, embeddings, RAG, AI engineering, and backend integrations.\n",
    "2. Next, prioritize Ayush‚Äôs expertise in AI tools, machine learning workflows, automation, and software engineering.\n",
    "3. Then focus on frontend development, especially React, React Native, Tailwind, Next.js, animations, UI/UX, and component-level design.\n",
    "4. After that, cover Ayush‚Äôs experience in game development, Unity, Android Studio, and other technical domains.\n",
    "5. For topics outside these areas, answer normally‚Äîclearly and helpfully.\n",
    "\n",
    "Rules:\n",
    "- Always stay friendly, supportive, and knowledgeable.\n",
    "- If the question relates to Ayush‚Äôs skills, answer with authority.\n",
    "- If you don‚Äôt know something or the context doesn't include it, say so.\n",
    "- Use the context below whenever it contains relevant information.\n",
    "- Never invent facts.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history):\n",
    "    docs = retriever.invoke(question) #searches your vector database.Jo question se related documents hote hain, woh laata hai.\n",
    "    context = \"\\n\\n\".join(doc.page_content for doc in docs) #CONTEXT THAT WE WILL GIVE TO OUR MODEL BY COMBINIG IT \n",
    "    system_prompt = SYSTEM_PROMPT_TEMPLATE.format(context=context)\n",
    "    response = llm.invoke([SystemMessage(content=system_prompt), HumanMessage(content=question)])\n",
    "    return response.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey! I'm Ayush's AI assistant‚Äîhere to help with anything about his work, skills, or projects.  \\n\\nLinkedIn: https://www.linkedin.com/in/ayush-tyagi-0a3694267\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_question(\"linkdin of aayush?\", []) #i.e fuzzy matching because of vd and rags \n",
    "#openAi models maybe way better "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What could possibly come next? üòÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\gradio\\chat_interface.py:348: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gr.ChatInterface(answer_question).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Admit it - you thought RAG would be more complicated than that!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
