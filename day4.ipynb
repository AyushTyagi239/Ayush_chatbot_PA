{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be0d1141",
   "metadata": {},
   "source": [
    "# RAG Day 4\n",
    "\n",
    "## Evaluation!\n",
    "\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Keep in mind how you would evaluate RAG for your business</h2>\n",
    "            <span style=\"color:#181;\">This is such an important part of building an accurate and reliable RAG pipeline. And it's applicable to many aspects of solving business problems with LLMs. People are often focused on RAG architecture and RAG frameworks for their business. But even more important: evaluations!</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60995166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bbcfcf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tests = test.load_tests()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd4d88a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65fd09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What backend technologies is Ayush learning?\n",
      "direct_fact\n",
      "Ayush is currently learning Node.js and Express.js for backend development, and has experience with Next.js API Routes.\n",
      "['Node.js', 'Express.js', 'backend']\n"
     ]
    }
   ],
   "source": [
    "example = tests[3]\n",
    "print(example.question)\n",
    "print(example.category)\n",
    "print(example.reference_answer)\n",
    "print(example.keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f058ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'direct_fact': 54,\n",
       "         'holistic': 10,\n",
       "         'spanning': 9,\n",
       "         'numerical': 6,\n",
       "         'relationship': 5,\n",
       "         'comparative': 4,\n",
       "         'temporal': 3})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "count = Counter([t.category for t in tests])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b413b186",
   "metadata": {},
   "outputs": [],
   "source": [
    "from evaluation.eval import evaluate_retrieval, evaluate_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "daca435e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RetrievalEval(mrr=0.2333333333333333, ndcg=0.46312815930610524, keywords_found=3, total_keywords=3, keyword_coverage=100.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_retrieval(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "925b37d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=qwen/qwen3-next-80b-a3b-instruct\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBadRequestError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28meval\u001b[39m, answer, chunks = \u001b[43mevaluate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\ayushchatbot\\week5\\evaluation\\eval.py:138\u001b[39m, in \u001b[36mevaluate_answer\u001b[39m\u001b[34m(test)\u001b[39m\n\u001b[32m    104\u001b[39m generated_answer, retrieved_docs = answer_question(test.question)\n\u001b[32m    106\u001b[39m judge_messages = [\n\u001b[32m    107\u001b[39m     {\n\u001b[32m    108\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m     }\n\u001b[32m    136\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m judge_response = \u001b[43mlitellm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMODEL\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjudge_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mAnswerEval\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m evaluation = AnswerEval.model_validate_json(\n\u001b[32m    145\u001b[39m     judge_response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m    146\u001b[39m )\n\u001b[32m    148\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m evaluation, generated_answer, retrieved_docs\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\.venv\\Lib\\site-packages\\litellm\\utils.py:1371\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1367\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logging_obj:\n\u001b[32m   1368\u001b[39m     logging_obj.failure_handler(\n\u001b[32m   1369\u001b[39m         e, traceback_exception, start_time, end_time\n\u001b[32m   1370\u001b[39m     )  \u001b[38;5;66;03m# DO NOT MAKE THREADED - router retry fallback relies on this!\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1371\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\.venv\\Lib\\site-packages\\litellm\\utils.py:1244\u001b[39m, in \u001b[36mclient.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1242\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1243\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1244\u001b[39m result = \u001b[43moriginal_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1245\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1246\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1247\u001b[39m     kwargs=kwargs,\n\u001b[32m   1248\u001b[39m     call_type=call_type,\n\u001b[32m   1249\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\.venv\\Lib\\site-packages\\litellm\\main.py:3767\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   3764\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[32m   3765\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   3766\u001b[39m     \u001b[38;5;66;03m## Map to OpenAI Exception\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3767\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_type(\n\u001b[32m   3768\u001b[39m         model=model,\n\u001b[32m   3769\u001b[39m         custom_llm_provider=custom_llm_provider,\n\u001b[32m   3770\u001b[39m         original_exception=e,\n\u001b[32m   3771\u001b[39m         completion_kwargs=args,\n\u001b[32m   3772\u001b[39m         extra_kwargs=kwargs,\n\u001b[32m   3773\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\.venv\\Lib\\site-packages\\litellm\\main.py:1172\u001b[39m, in \u001b[36mcompletion\u001b[39m\u001b[34m(model, messages, timeout, temperature, top_p, n, stream, stream_options, stop, max_completion_tokens, max_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, reasoning_effort, response_format, seed, tools, tool_choice, logprobs, top_logprobs, parallel_tool_calls, web_search_options, deployment_id, extra_headers, safety_identifier, service_tier, functions, function_call, base_url, api_version, api_key, model_list, thinking, shared_session, **kwargs)\u001b[39m\n\u001b[32m   1170\u001b[39m     model = deployment_id\n\u001b[32m   1171\u001b[39m     custom_llm_provider = \u001b[33m\"\u001b[39m\u001b[33mazure\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1172\u001b[39m model, custom_llm_provider, dynamic_api_key, api_base = \u001b[43mget_llm_provider\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m provider_specific_header \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1180\u001b[39m     headers.update(\n\u001b[32m   1181\u001b[39m         ProviderSpecificHeaderUtils.get_provider_specific_headers(\n\u001b[32m   1182\u001b[39m             provider_specific_header=provider_specific_header,\n\u001b[32m   1183\u001b[39m             custom_llm_provider=custom_llm_provider,\n\u001b[32m   1184\u001b[39m         )\n\u001b[32m   1185\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:421\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    420\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, litellm.exceptions.BadRequestError):\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    422\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    423\u001b[39m         error_str = (\n\u001b[32m    424\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGetLLMProvider Exception - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33moriginal model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    425\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\project\\llm_engineering\\.venv\\Lib\\site-packages\\litellm\\litellm_core_utils\\get_llm_provider_logic.py:398\u001b[39m, in \u001b[36mget_llm_provider\u001b[39m\u001b[34m(model, custom_llm_provider, api_base, api_key, litellm_params)\u001b[39m\n\u001b[32m    396\u001b[39m     error_str = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m Pass model as E.g. For \u001b[39m\u001b[33m'\u001b[39m\u001b[33mHuggingface\u001b[39m\u001b[33m'\u001b[39m\u001b[33m inference endpoints pass in `completion(model=\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhuggingface/starcoder\u001b[39m\u001b[33m'\u001b[39m\u001b[33m,..)` Learn more: https://docs.litellm.ai/docs/providers\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    397\u001b[39m     \u001b[38;5;66;03m# maps to openai.NotFoundError, this is raised when openai does not recognize the llm\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m litellm.exceptions.BadRequestError(  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    399\u001b[39m         message=error_str,\n\u001b[32m    400\u001b[39m         model=model,\n\u001b[32m    401\u001b[39m         response=httpx.Response(\n\u001b[32m    402\u001b[39m             status_code=\u001b[32m400\u001b[39m,\n\u001b[32m    403\u001b[39m             content=error_str,\n\u001b[32m    404\u001b[39m             request=httpx.Request(method=\u001b[33m\"\u001b[39m\u001b[33mcompletion\u001b[39m\u001b[33m\"\u001b[39m, url=\u001b[33m\"\u001b[39m\u001b[33mhttps://github.com/BerriAI/litellm\u001b[39m\u001b[33m\"\u001b[39m),  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m    405\u001b[39m         ),\n\u001b[32m    406\u001b[39m         llm_provider=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    407\u001b[39m     )\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m api_base \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(api_base, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\n\u001b[32m    410\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mapi base needs to be a string. api_base=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m\"\u001b[39m.format(api_base)\n\u001b[32m    411\u001b[39m     )\n",
      "\u001b[31mBadRequestError\u001b[39m: litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model=qwen/qwen3-next-80b-a3b-instruct\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
     ]
    }
   ],
   "source": [
    "eval, answer, chunks = evaluate_answer(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01965312",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cd34561",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'feedback'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43meval\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeedback\u001b[49m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28meval\u001b[39m.accuracy)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28meval\u001b[39m.completeness)\n",
      "\u001b[31mAttributeError\u001b[39m: 'builtin_function_or_method' object has no attribute 'feedback'"
     ]
    }
   ],
   "source": [
    "print(eval.feedback)\n",
    "print(eval.accuracy)\n",
    "print(eval.completeness)\n",
    "print(eval.relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5e0cd4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-engineering",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
