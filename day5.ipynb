{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c26c848",
   "metadata": {},
   "source": [
    "# Let's go PRO!\n",
    "\n",
    "Advanced RAG Techniques!\n",
    "\n",
    "Let's start by digging into ingest:\n",
    "\n",
    "1. No LangChain! Just native for maximum flexibility\n",
    "2. Let's use an LLM to divide up chunks in a sensible way\n",
    "3. Let's use the best chunk size and encoder from yesterday\n",
    "4. Let's also have the LLM rewrite chunks in a way that's most useful (\"document pre-processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e9f5f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Hi! I'm a large language model, specifically based on the transformer architecture.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel, Field\n",
    "from chromadb import PersistentClient\n",
    "from tqdm import tqdm\n",
    "from litellm import completion\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# FIXED: added provider prefix\n",
    "MODEL = \"nvidia_nim/meta/llama-3.1-8b-instruct\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "DB_NAME = \"preprocessed_db\"\n",
    "collection_name = \"docs\"\n",
    "embedding_model = \"text-embedding-3-large\"\n",
    "KNOWLEDGE_BASE_PATH = Path(\"knowledge-base\")\n",
    "AVERAGE_CHUNK_SIZE = 400\n",
    "\n",
    "#openai = OpenAI()\n",
    "# 4) quick test call\n",
    "messages = [{\"role\":\"user\", \"content\":\"Say hi and identify the model used in one short sentence.\"}]\n",
    "\n",
    "resp = completion(model=MODEL, messages=messages)\n",
    "\n",
    "print(\"Status:\", resp.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfef3fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA_API_KEY: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(override=True)\n",
    "print(\"NVIDIA_API_KEY:\", bool(os.getenv(\"NVIDIA_API_KEY\")))  # prints True if present, False if missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bfac66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by LangChain's Document - let's have something similar\n",
    "\n",
    "class Result(BaseModel):\n",
    "    page_content: str\n",
    "    metadata: dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9d0b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to perfectly represent a chunk\n",
    "\n",
    "class Chunk(BaseModel):\n",
    "    headline: str = Field(description=\"A brief heading for this chunk, typically a few words, that is most likely to be surfaced in a query\")\n",
    "    summary: str = Field(description=\"A few sentences summarizing the content of this chunk to answer common questions\")\n",
    "    original_text: str = Field(description=\"The original text of this chunk from the provided document, exactly as is, not changed in any way\")\n",
    "\n",
    "    def as_result(self, document):\n",
    "        metadata = {\"source\": document[\"source\"], \"type\": document[\"type\"]}\n",
    "        return Result(page_content=self.headline + \"\\n\\n\" + self.summary + \"\\n\\n\" + self.original_text,metadata=metadata)\n",
    "\n",
    "\n",
    "class Chunks(BaseModel):\n",
    "    chunks: list[Chunk]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284b64c1",
   "metadata": {},
   "source": [
    "## Three steps:\n",
    "\n",
    "1. Fetch documents from the knowledge base, like LangChain did\n",
    "2. Call an LLM to turn documents into Chunks\n",
    "3. Store the Chunks in Chroma\n",
    "\n",
    "That's it!\n",
    "\n",
    "### Let's start with Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db5abdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents():\n",
    "    \"\"\"A homemade version of the LangChain DirectoryLoader\"\"\"\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for folder in KNOWLEDGE_BASE_PATH.iterdir():\n",
    "        doc_type = folder.name\n",
    "        for file in folder.rglob(\"*.md\"):\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                documents.append({\"type\": doc_type, \"source\": file.as_posix(), \"text\": f.read()})\n",
    "\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fe0a213",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 30 documents\n"
     ]
    }
   ],
   "source": [
    "documents = fetch_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffa1c68",
   "metadata": {},
   "source": [
    "### Donezo! On to Step 2 - make the chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "900e4170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(document):\n",
    "    how_many = (len(document[\"text\"]) // AVERAGE_CHUNK_SIZE) + 1\n",
    "    return f\"\"\"\n",
    "You are an expert RAG pre-processor whose job is to split documents into high-quality, overlapping chunks for Ayush's Personal Knowledge Base.\n",
    "\n",
    "**Owner:** Ayush Tyagi  \n",
    "**Document Type:** {document[\"type\"]}  \n",
    "**Source Location:** {document[\"source\"]}\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Your Goal\n",
    "Split the document into clean, retrieval-optimized chunks that will be used by Ayushâ€™s personal chatbot to answer questions about his life, projects, interests, and background.\n",
    "\n",
    "### ðŸ“Œ Required Guidelines\n",
    "1. **Cover the entire document. Do not omit any content.**\n",
    "2. The document should ideally be split into **~{how_many} chunks** (but choose more or fewer if needed for quality).\n",
    "3. Use **overlapping chunks**:\n",
    "   - About **25% overlap**, OR\n",
    "   - About **50 words** of shared text.\n",
    "4. Each chunk must include:\n",
    "   - **Chunk Title / Headline**\n",
    "   - **Chunk Summary** (2â€“4 sentences)\n",
    "   - **Chunk Text** (the exact original text of the chunk)\n",
    "\n",
    "### ðŸ“Œ Quality Requirements\n",
    "- Chunks must be coherent and readable.\n",
    "- Do not cut sentences in unnatural places.\n",
    "- Ensure important sections appear in multiple chunks via overlap.\n",
    "- Maintain the original order of the document.\n",
    "- Do NOT invent or alter information. Use only the text given.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“„ Document to Chunk:\n",
    "\n",
    "{document[\"text\"]}\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Output Format (IMPORTANT)\n",
    "\n",
    "Produce the chunks in the following structure:\n",
    "\n",
    "CHUNK 1  \n",
    "Headline: <title>  \n",
    "Summary: <summary>  \n",
    "Text: <original chunked text>\n",
    "\n",
    "CHUNK 2  \n",
    "Headline: <title>  \n",
    "Summary: <summary>  \n",
    "Text: <original chunked text>\n",
    "\n",
    "(Continue for all chunks)\n",
    "\n",
    "---\n",
    "\n",
    "Now create the chunks.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38103b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert RAG pre-processor whose job is to split documents into high-quality, overlapping chunks for Ayush's Personal Knowledge Base.\n",
      "\n",
      "**Owner:** Ayush Tyagi  \n",
      "**Document Type:** extras  \n",
      "**Source Location:** knowledge-base/extras/achievements.md\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ“Œ Your Goal\n",
      "Split the document into clean, retrieval-optimized chunks that will be used by Ayushâ€™s personal chatbot to answer questions about his life, projects, interests, and background.\n",
      "\n",
      "### ðŸ“Œ Required Guidelines\n",
      "1. **Cover the entire document. Do not omit any content.**\n",
      "2. The document should ideally be split into **~6 chunks** (but choose more or fewer if needed for quality).\n",
      "3. Use **overlapping chunks**:\n",
      "   - About **25% overlap**, OR\n",
      "   - About **50 words** of shared text.\n",
      "4. Each chunk must include:\n",
      "   - **Chunk Title / Headline**\n",
      "   - **Chunk Summary** (2â€“4 sentences)\n",
      "   - **Chunk Text** (the exact original text of the chunk)\n",
      "\n",
      "### ðŸ“Œ Quality Requirements\n",
      "- Chunks must be coherent and readable.\n",
      "- Do not cut sentences in unnatural places.\n",
      "- Ensure important sections appear in multiple chunks via overlap.\n",
      "- Maintain the original order of the document.\n",
      "- Do NOT invent or alter information. Use only the text given.\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ“„ Document to Chunk:\n",
      "\n",
      "# Achievements & Milestones\n",
      "\n",
      "## Technical Achievements\n",
      "### Game Development\n",
      "- **Multiple Shipped Games:** Successfully developed and published 4 complete games across different genres\n",
      "- **Unity Proficiency:** Achieved advanced level in Unity game engine with both 2D and 3D projects\n",
      "- **Physics Implementation:** Mastered game physics for realistic vehicle and character movement\n",
      "\n",
      "### Web Development\n",
      "- **Full-Stack Projects:** Built multiple complete web applications from frontend to backend\n",
      "- **Modern Frameworks:** Gained expertise in React, Next.js, and modern web technologies\n",
      "- **Responsive Design:** Created applications optimized for all devices and screen sizes\n",
      "\n",
      "### AI/ML Engineering\n",
      "- **LLM Projects:** Developed advanced AI applications including code conversion and multimodal assistants\n",
      "- **Practical AI Applications:** Successfully implemented AI solutions for real-world problems\n",
      "- **Learning Acceleration:** Quickly mastered complex AI concepts and technologies\n",
      "\n",
      "## Academic & Learning Achievements\n",
      "- **Consistent Learning:** Completed multiple comprehensive technical certifications while maintaining academic performance\n",
      "- **Self-Taught Expertise:** Developed advanced skills through independent learning and project work\n",
      "- **Practical Application:** Successfully applied theoretical knowledge to real projects\n",
      "\n",
      "## Project Milestones\n",
      "### Game Development\n",
      "- Shipped 4 complete games across different genres (platformer, shooter, racing, simulator)\n",
      "- Mastered both 2D and 3D game development in Unity\n",
      "- Implemented complex game mechanics including physics, AI, and animations\n",
      "\n",
      "### Web Development\n",
      "- Built and deployed multiple full-stack web applications\n",
      "- Mastered modern React ecosystem and best practices\n",
      "- Created responsive, user-friendly interfaces\n",
      "\n",
      "### AI Engineering\n",
      "- Developed working AI applications with practical utility\n",
      "- Implemented complex AI systems including multimodal interfaces\n",
      "- Applied cutting-edge AI techniques to real problems\n",
      "\n",
      "## Personal Growth\n",
      "- **Rapid Skill Acquisition:** Demonstrated ability to quickly learn and apply new technologies\n",
      "- **Project Completion:** Strong track record of finishing complex projects\n",
      "- **Problem-Solving:** Developed systematic approach to technical challenges\n",
      "- **Creativity & Innovation:** Consistently added unique features and improvements to projects\n",
      "\n",
      "---\n",
      "\n",
      "### ðŸ“Œ Output Format (IMPORTANT)\n",
      "\n",
      "Produce the chunks in the following structure:\n",
      "\n",
      "CHUNK 1  \n",
      "Headline: <title>  \n",
      "Summary: <summary>  \n",
      "Text: <original chunked text>\n",
      "\n",
      "CHUNK 2  \n",
      "Headline: <title>  \n",
      "Summary: <summary>  \n",
      "Text: <original chunked text>\n",
      "\n",
      "(Continue for all chunks)\n",
      "\n",
      "---\n",
      "\n",
      "Now create the chunks.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(make_prompt(documents[9]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f58850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_messages(document): # call thsi and put in lisrt of dics\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": make_prompt(document)},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ab04779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': \"\\nYou are an expert RAG pre-processor whose job is to split documents into high-quality, overlapping chunks for Ayush's Personal Knowledge Base.\\n\\n**Owner:** Ayush Tyagi  \\n**Document Type:** extras  \\n**Source Location:** knowledge-base/extras/achievements.md\\n\\n---\\n\\n### ðŸ“Œ Your Goal\\nSplit the document into clean, retrieval-optimized chunks that will be used by Ayushâ€™s personal chatbot to answer questions about his life, projects, interests, and background.\\n\\n### ðŸ“Œ Required Guidelines\\n1. **Cover the entire document. Do not omit any content.**\\n2. The document should ideally be split into **~6 chunks** (but choose more or fewer if needed for quality).\\n3. Use **overlapping chunks**:\\n   - About **25% overlap**, OR\\n   - About **50 words** of shared text.\\n4. Each chunk must include:\\n   - **Chunk Title / Headline**\\n   - **Chunk Summary** (2â€“4 sentences)\\n   - **Chunk Text** (the exact original text of the chunk)\\n\\n### ðŸ“Œ Quality Requirements\\n- Chunks must be coherent and readable.\\n- Do not cut sentences in unnatural places.\\n- Ensure important sections appear in multiple chunks via overlap.\\n- Maintain the original order of the document.\\n- Do NOT invent or alter information. Use only the text given.\\n\\n---\\n\\n### ðŸ“„ Document to Chunk:\\n\\n# Achievements & Milestones\\n\\n## Technical Achievements\\n### Game Development\\n- **Multiple Shipped Games:** Successfully developed and published 4 complete games across different genres\\n- **Unity Proficiency:** Achieved advanced level in Unity game engine with both 2D and 3D projects\\n- **Physics Implementation:** Mastered game physics for realistic vehicle and character movement\\n\\n### Web Development\\n- **Full-Stack Projects:** Built multiple complete web applications from frontend to backend\\n- **Modern Frameworks:** Gained expertise in React, Next.js, and modern web technologies\\n- **Responsive Design:** Created applications optimized for all devices and screen sizes\\n\\n### AI/ML Engineering\\n- **LLM Projects:** Developed advanced AI applications including code conversion and multimodal assistants\\n- **Practical AI Applications:** Successfully implemented AI solutions for real-world problems\\n- **Learning Acceleration:** Quickly mastered complex AI concepts and technologies\\n\\n## Academic & Learning Achievements\\n- **Consistent Learning:** Completed multiple comprehensive technical certifications while maintaining academic performance\\n- **Self-Taught Expertise:** Developed advanced skills through independent learning and project work\\n- **Practical Application:** Successfully applied theoretical knowledge to real projects\\n\\n## Project Milestones\\n### Game Development\\n- Shipped 4 complete games across different genres (platformer, shooter, racing, simulator)\\n- Mastered both 2D and 3D game development in Unity\\n- Implemented complex game mechanics including physics, AI, and animations\\n\\n### Web Development\\n- Built and deployed multiple full-stack web applications\\n- Mastered modern React ecosystem and best practices\\n- Created responsive, user-friendly interfaces\\n\\n### AI Engineering\\n- Developed working AI applications with practical utility\\n- Implemented complex AI systems including multimodal interfaces\\n- Applied cutting-edge AI techniques to real problems\\n\\n## Personal Growth\\n- **Rapid Skill Acquisition:** Demonstrated ability to quickly learn and apply new technologies\\n- **Project Completion:** Strong track record of finishing complex projects\\n- **Problem-Solving:** Developed systematic approach to technical challenges\\n- **Creativity & Innovation:** Consistently added unique features and improvements to projects\\n\\n---\\n\\n### ðŸ“Œ Output Format (IMPORTANT)\\n\\nProduce the chunks in the following structure:\\n\\nCHUNK 1  \\nHeadline: <title>  \\nSummary: <summary>  \\nText: <original chunked text>\\n\\nCHUNK 2  \\nHeadline: <title>  \\nSummary: <summary>  \\nText: <original chunked text>\\n\\n(Continue for all chunks)\\n\\n---\\n\\nNow create the chunks.\\n\"}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_messages(documents[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ea20aba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from litellm import acompletion\n",
    "\n",
    "async def process_document_async(document):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"\n",
    "You MUST respond ONLY with valid JSON in the following exact format:\n",
    "\n",
    "{\n",
    "  \"chunks\": [\n",
    "    {\n",
    "      \"headline\": \"...\",\n",
    "      \"summary\": \"...\",\n",
    "      \"text\": \"...\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "No explanations, no extra text, no formatting outside the JSON.\n",
    "If the content is too long, split it cleanly into multiple chunks.\n",
    "\"\"\"\n",
    "        },\n",
    "        *make_messages(document)\n",
    "    ]\n",
    "\n",
    "    response = await acompletion(\n",
    "        model=MODEL,\n",
    "        messages=messages,\n",
    "        response_format={\"type\": \"json_object\"}\n",
    "    )\n",
    "\n",
    "    reply = response.choices[0].message.content\n",
    "    return Chunks.model_validate_json(reply).chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c0730f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MOONSHOT_API_KEY...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "FAILED: litellm.AuthenticationError: AuthenticationError: MoonshotException - Invalid Authentication\n",
      "----------------------------------------\n",
      "Testing kimi_k2_api_key...\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
      "\n",
      "FAILED: litellm.AuthenticationError: AuthenticationError: MoonshotException - Invalid Authentication\n"
     ]
    }
   ],
   "source": [
    "from litellm import completion\n",
    "import os\n",
    "\n",
    "model = \"moonshot/kimi-k2\"\n",
    "\n",
    "try:\n",
    "    print(\"Testing MOONSHOT_API_KEY...\")\n",
    "    os.environ[\"MOONSHOT_API_KEY\"] = \"nvapi-JHaTD3YaBlSBBmJQ-V_LOM8fAnOWXtDoML6bypoQCRVYm\"\n",
    "    resp = completion(model=model, messages=[{\"role\": \"user\", \"content\": \"hello\"}])\n",
    "    print(\"MOONSHOT_API_KEY WORKS!\")\n",
    "    print(resp.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"FAILED:\", e)\n",
    "\n",
    "print(\"-\" * 40)\n",
    "\n",
    "try:\n",
    "    print(\"Testing kimi_k2_api_key...\")\n",
    "    os.environ[\"MOONSHOT_API_KEY\"] = \"nvapi-JHaTD3YaBlSBBmJQ-V_LOM8fAnOWXtDoML6bypoGHYItOnaRqGOw5lYm\"\n",
    "    resp = completion(model=model, messages=[{\"role\": \"user\", \"content\": \"hello\"}])\n",
    "    print(\"kimi_k2_api_key WORKS!\")\n",
    "    print(resp.choices[0].message.content)\n",
    "except Exception as e:\n",
    "    print(\"FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "168712ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def create_chunks_parallel(documents):\n",
    "    tasks = [process_document_async(doc) for doc in documents]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "\n",
    "    # flatten\n",
    "    all_chunks = [chunk for doc_chunks in results for chunk in doc_chunks]\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480494d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_document(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccab1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_chunks(documents):\n",
    "#     chunks = []\n",
    "#     for doc in tqdm(documents):\n",
    "#         chunks.extend(process_document(doc))\n",
    "#     return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93115f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks = create_chunks(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8f2e227e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\ast.py:50: RuntimeWarning: coroutine 'create_chunks_parallel' was never awaited\n",
      "  return compile(source, filename, mode, flags,\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "litellm.RateLimitError: RateLimitError: Nvidia_nimException - Error code: 429 - {'status': 429, 'title': 'Too Many Requests'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:823\u001b[39m, in \u001b[36mOpenAIChatCompletion.acompletion\u001b[39m\u001b[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001b[39m\n\u001b[32m    810\u001b[39m logging_obj.pre_call(\n\u001b[32m    811\u001b[39m     \u001b[38;5;28minput\u001b[39m=data[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    812\u001b[39m     api_key=openai_aclient.api_key,\n\u001b[32m   (...)\u001b[39m\u001b[32m    820\u001b[39m     },\n\u001b[32m    821\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m headers, response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.make_openai_chat_completion_request(\n\u001b[32m    824\u001b[39m     openai_aclient=openai_aclient,\n\u001b[32m    825\u001b[39m     data=data,\n\u001b[32m    826\u001b[39m     timeout=timeout,\n\u001b[32m    827\u001b[39m     logging_obj=logging_obj,\n\u001b[32m    828\u001b[39m )\n\u001b[32m    829\u001b[39m stringified_response = response.model_dump()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\litellm_core_utils\\logging_utils.py:190\u001b[39m, in \u001b[36mtrack_llm_api_timing.<locals>.decorator.<locals>.async_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    191\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:454\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_aclient, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m--> \u001b[39m\u001b[32m454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:436\u001b[39m, in \u001b[36mOpenAIChatCompletion.make_openai_chat_completion_request\u001b[39m\u001b[34m(self, openai_aclient, data, timeout, logging_obj)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    435\u001b[39m     raw_response = (\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m openai_aclient.chat.completions.with_raw_response.create(\n\u001b[32m    437\u001b[39m             **data, timeout=timeout\n\u001b[32m    438\u001b[39m         )\n\u001b[32m    439\u001b[39m     )\n\u001b[32m    440\u001b[39m     end_time = time.time()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_legacy_response.py:381\u001b[39m, in \u001b[36masync_to_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    379\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2672\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2671\u001b[39m validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2672\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2673\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2674\u001b[39m     body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2675\u001b[39m         {\n\u001b[32m   2676\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2677\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2678\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2679\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2680\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2681\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2682\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2683\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2684\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2685\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2686\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2687\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2688\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2689\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2690\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2691\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2692\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2693\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_retention\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_retention,\n\u001b[32m   2694\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2695\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2696\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2697\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2698\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2699\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2700\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2701\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2702\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2703\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2704\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2705\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2706\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2707\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2708\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2709\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2710\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2711\u001b[39m         },\n\u001b[32m   2712\u001b[39m         completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2713\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2714\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2715\u001b[39m     ),\n\u001b[32m   2716\u001b[39m     options=make_request_options(\n\u001b[32m   2717\u001b[39m         extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2718\u001b[39m     ),\n\u001b[32m   2719\u001b[39m     cast_to=ChatCompletion,\n\u001b[32m   2720\u001b[39m     stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2721\u001b[39m     stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2722\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1791\u001b[39m opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m     method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\openai\\_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1593\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'status': 429, 'title': 'Too Many Requests'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOpenAIError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\main.py:599\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m asyncio.iscoroutine(init_response):\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m init_response\n\u001b[32m    600\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\llms\\openai\\openai.py:870\u001b[39m, in \u001b[36mOpenAIChatCompletion.acompletion\u001b[39m\u001b[34m(self, messages, optional_params, litellm_params, provider_config, model, model_response, logging_obj, timeout, api_key, api_base, api_version, organization, client, max_retries, headers, drop_params, stream_options, fake_stream, shared_session)\u001b[39m\n\u001b[32m    868\u001b[39m message = \u001b[38;5;28mgetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(e))\n\u001b[32m--> \u001b[39m\u001b[32m870\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[32m    871\u001b[39m     status_code=status_code,\n\u001b[32m    872\u001b[39m     message=message,\n\u001b[32m    873\u001b[39m     headers=error_headers,\n\u001b[32m    874\u001b[39m     body=exception_body,\n\u001b[32m    875\u001b[39m )\n",
      "\u001b[31mOpenAIError\u001b[39m: Error code: 429 - {'status': 429, 'title': 'Too Many Requests'}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m chunks = \u001b[38;5;28;01mawait\u001b[39;00m create_chunks_parallel(documents)\n\u001b[32m      2\u001b[39m chunks[:\u001b[32m2\u001b[39m]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mcreate_chunks_parallel\u001b[39m\u001b[34m(documents)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate_chunks_parallel\u001b[39m(documents):\n\u001b[32m      2\u001b[39m     tasks = [process_document_async(doc) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     results = \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# flatten\u001b[39;00m\n\u001b[32m      6\u001b[39m     all_chunks = [chunk \u001b[38;5;28;01mfor\u001b[39;00m doc_chunks \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m doc_chunks]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mprocess_document_async\u001b[39m\u001b[34m(document)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprocess_document_async\u001b[39m(document):\n\u001b[32m      5\u001b[39m     messages = [\n\u001b[32m      6\u001b[39m         {\n\u001b[32m      7\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m         *make_messages(document)\n\u001b[32m     26\u001b[39m     ]\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m acompletion(\n\u001b[32m     29\u001b[39m         model=MODEL,\n\u001b[32m     30\u001b[39m         messages=messages,\n\u001b[32m     31\u001b[39m         response_format={\u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mjson_object\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m     32\u001b[39m     )\n\u001b[32m     34\u001b[39m     reply = response.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Chunks.model_validate_json(reply).chunks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\utils.py:1642\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1640\u001b[39m timeout = _get_wrapper_timeout(kwargs=kwargs, exception=e)\n\u001b[32m   1641\u001b[39m \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1642\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\utils.py:1488\u001b[39m, in \u001b[36mclient.<locals>.wrapper_async\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1485\u001b[39m         print_verbose(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError while checking max token limit: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1487\u001b[39m \u001b[38;5;66;03m# MODEL CALL\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1488\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m original_function(*args, **kwargs)\n\u001b[32m   1489\u001b[39m end_time = datetime.datetime.now()\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _is_streaming_request(\n\u001b[32m   1491\u001b[39m     kwargs=kwargs,\n\u001b[32m   1492\u001b[39m     call_type=call_type,\n\u001b[32m   1493\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\main.py:618\u001b[39m, in \u001b[36macompletion\u001b[39m\u001b[34m(model, messages, functions, function_call, timeout, temperature, top_p, n, stream, stream_options, stop, max_tokens, max_completion_tokens, modalities, prediction, audio, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, parallel_tool_calls, logprobs, top_logprobs, deployment_id, reasoning_effort, verbosity, safety_identifier, service_tier, base_url, api_version, api_key, model_list, extra_headers, thinking, web_search_options, shared_session, **kwargs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    617\u001b[39m     custom_llm_provider = custom_llm_provider \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m618\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mexception_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_llm_provider\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m        \u001b[49m\u001b[43moriginal_exception\u001b[49m\u001b[43m=\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompletion_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    623\u001b[39m \u001b[43m        \u001b[49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:2328\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m   2326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m exception_mapping_worked:\n\u001b[32m   2327\u001b[39m     \u001b[38;5;28msetattr\u001b[39m(e, \u001b[33m\"\u001b[39m\u001b[33mlitellm_response_headers\u001b[39m\u001b[33m\"\u001b[39m, litellm_response_headers)\n\u001b[32m-> \u001b[39m\u001b[32m2328\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   2329\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2330\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m error_type \u001b[38;5;129;01min\u001b[39;00m litellm.LITELLM_EXCEPTION_TYPES:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ayush\\AppData\\Local\\anaconda3\\envs\\llms\\Lib\\site-packages\\litellm\\litellm_core_utils\\exception_mapping_utils.py:355\u001b[39m, in \u001b[36mexception_type\u001b[39m\u001b[34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ExceptionCheckers.is_error_str_rate_limit(error_str):\n\u001b[32m    354\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RateLimitError(\n\u001b[32m    356\u001b[39m         message=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRateLimitError: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexception_provider\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    357\u001b[39m         model=model,\n\u001b[32m    358\u001b[39m         llm_provider=custom_llm_provider,\n\u001b[32m    359\u001b[39m         response=\u001b[38;5;28mgetattr\u001b[39m(original_exception, \u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    360\u001b[39m     )\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m ExceptionCheckers.is_error_str_context_window_exceeded(error_str):\n\u001b[32m    362\u001b[39m     exception_mapping_worked = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRateLimitError\u001b[39m: litellm.RateLimitError: RateLimitError: Nvidia_nimException - Error code: 429 - {'status': 429, 'title': 'Too Many Requests'}"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chunks = await create_chunks_parallel(documents)\n",
    "chunks[:2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f51544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9750104c",
   "metadata": {},
   "source": [
    "### Well that was easy! If a bit slow.\n",
    "\n",
    "In the python module version, I sneakily use the multi-processing Pool to run this in parallel,\n",
    "but if you get a Rate Limit Error you can turn this off in the code.\n",
    "\n",
    "### Finally, Step 3 - save the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f36b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(chunks):\n",
    "    chroma = PersistentClient(path=DB_NAME)\n",
    "    if collection_name in [c.name for c in chroma.list_collections()]:\n",
    "        chroma.delete_collection(collection_name)\n",
    "\n",
    "    texts = [chunk.page_content for chunk in chunks]\n",
    "    emb = openai.embeddings.create(model=embedding_model, input=texts).data\n",
    "    vectors = [e.embedding for e in emb]\n",
    "\n",
    "    collection = chroma.get_or_create_collection(collection_name)\n",
    "\n",
    "    ids = [str(i) for i in range(len(chunks))]\n",
    "    metas = [chunk.metadata for chunk in chunks]\n",
    "\n",
    "    collection.add(ids=ids, embeddings=vectors, documents=texts, metadatas=metas)\n",
    "    print(f\"Vectorstore created with {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f52038",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_embeddings(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf738d0",
   "metadata": {},
   "source": [
    "# Nothing more to do here... right?\n",
    "\n",
    "Wait! Didja think I'd forget??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c318a46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma = PersistentClient(path=DB_NAME)\n",
    "collection = chroma.get_or_create_collection(collection_name)\n",
    "result = collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "vectors = np.array(result['embeddings'])\n",
    "documents = result['documents']\n",
    "metadatas = result['metadatas']\n",
    "doc_types = [metadata['type'] for metadata in metadatas]\n",
    "colors = [['blue', 'green', 'red', 'orange'][['products', 'employees', 'contracts', 'company'].index(t)] for t in doc_types]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4683c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 2D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(title='2D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x',yaxis_title='y'),\n",
    "    width=800,\n",
    "    height=600,\n",
    "    margin=dict(r=20, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=3, random_state=42)\n",
    "reduced_vectors = tsne.fit_transform(vectors)\n",
    "\n",
    "# Create the 3D scatter plot\n",
    "fig = go.Figure(data=[go.Scatter3d(\n",
    "    x=reduced_vectors[:, 0],\n",
    "    y=reduced_vectors[:, 1],\n",
    "    z=reduced_vectors[:, 2],\n",
    "    mode='markers',\n",
    "    marker=dict(size=5, color=colors, opacity=0.8),\n",
    "    text=[f\"Type: {t}<br>Text: {d[:100]}...\" for t, d in zip(doc_types, documents)],\n",
    "    hoverinfo='text'\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title='3D Chroma Vector Store Visualization',\n",
    "    scene=dict(xaxis_title='x', yaxis_title='y', zaxis_title='z'),\n",
    "    width=900,\n",
    "    height=700,\n",
    "    margin=dict(r=10, b=10, l=10, t=40)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72b54a",
   "metadata": {},
   "source": [
    "## And now - let's build an Advanced RAG!\n",
    "\n",
    "We will use these techniques:\n",
    "\n",
    "1. Reranking - reorder the rank results\n",
    "2. Query re-writing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a3818c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RankOrder(BaseModel): #pydantic object\n",
    "    order: list[int] = Field(\n",
    "        description=\"The order of relevance of chunks, from most relevant to least relevant, by chunk id number\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8446c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank(question, chunks):\n",
    "    system_prompt = \"\"\"\n",
    "You are a document re-ranker.\n",
    "You are provided with a question and a list of relevant chunks of text from a query of a knowledge base.\n",
    "The chunks are provided in the order they were retrieved; this should be approximately ordered by relevance, but you may be able to improve on that.\n",
    "You must rank order the provided chunks by relevance to the question, with the most relevant chunk first.\n",
    "Reply only with the list of ranked chunk ids, nothing else. Include all the chunk ids you are provided with, reranked.\n",
    "\"\"\"\n",
    "    user_prompt = f\"The user has asked the following question:\\n\\n{question}\\n\\nOrder all the chunks of text by relevance to the question, from most relevant to least relevant. Include all the chunk ids you are provided with, reranked.\\n\\n\"\n",
    "    user_prompt += \"Here are the chunks:\\n\\n\"\n",
    "    for index, chunk in enumerate(chunks):\n",
    "        user_prompt += f\"# CHUNK ID: {index + 1}:\\n\\n{chunk.page_content}\\n\\n\" #kisting uou chunks with list id \n",
    "    user_prompt += \"Reply only with the list of ranked chunk ids, nothing else.\"\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    response = completion(model=MODEL, messages=messages, response_format=RankOrder)\n",
    "    reply = response.choices[0].message.content\n",
    "    order = RankOrder.model_validate_json(reply).order\n",
    "    print(order)\n",
    "    return [chunks[i - 1] for i in order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78048d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RETRIEVAL_K = 10\n",
    "\n",
    "def fetch_context_unranked(question):\n",
    "    query = openai.embeddings.create(model=embedding_model, input=[question]).data[0].embedding\n",
    "    results = collection.query(query_embeddings=[query], n_results=RETRIEVAL_K)\n",
    "    chunks = []\n",
    "    for result in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        chunks.append(Result(page_content=result[0], metadata=result[1]))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53f6de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2ed5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who won the IIOTY award?\"\n",
    "chunks = fetch_context_unranked(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0794453f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in chunks:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7661e767",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23594f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in reranked:\n",
    "    print(chunk.page_content[:15]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405de4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Who went to Manchester University?\"\n",
    "RETRIEVAL_K = 20\n",
    "chunks = fetch_context_unranked(question)\n",
    "for index, c in enumerate(chunks):\n",
    "    if \"manchester\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9e343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22948df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, c in enumerate(reranked):\n",
    "    if \"manchester\" in c.page_content.lower():\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1cae50",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca740b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_context(question):\n",
    "    chunks = fetch_context_unranked(question)\n",
    "    return rerank(question, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a knowledgeable, friendly assistant representing the company Insurellm.\n",
    "You are chatting with a user about Insurellm.\n",
    "Your answer will be evaluated for accuracy, relevance and completeness, so make sure it only answers the question and fully answers it.\n",
    "If you don't know the answer, say so.\n",
    "For context, here are specific extracts from the Knowledge Base that might be directly relevant to the user's question:\n",
    "{context}\n",
    "\n",
    "With this context, please answer the user's question. Be accurate, relevant and complete.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1b5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the context, include the source of the chunk\n",
    "\n",
    "def make_rag_messages(question, history, chunks):\n",
    "    context = \"\\n\\n\".join(f\"Extract from {chunk.metadata['source']}:\\n{chunk.page_content}\" for chunk in chunks)\n",
    "    system_prompt = SYSTEM_PROMPT.format(context=context)\n",
    "    return [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": question}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d9491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(question, history=[]):\n",
    "    \"\"\"Rewrite the user's question to be a more specific question that is more likely to surface relevant content in the Knowledge Base.\"\"\"\n",
    "    message = f\"\"\"\n",
    "You are in a conversation with a user, answering questions about the company Insurellm.\n",
    "You are about to look up information in a Knowledge Base to answer the user's question.\n",
    "\n",
    "This is the history of your conversation so far with the user:\n",
    "{history}\n",
    "\n",
    "And this is the user's current question:\n",
    "{question}\n",
    "\n",
    "Respond only with a single, refined question that you will use to search the Knowledge Base.\n",
    "It should be a VERY short specific question most likely to surface content. Focus on the question details.\n",
    "Don't mention the company name unless it's a general question about the company.\n",
    "IMPORTANT: Respond ONLY with the knowledgebase query, nothing else.\n",
    "\"\"\"\n",
    "    response = completion(model=MODEL, messages=[{\"role\": \"system\", \"content\": message}])\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d050a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rewrite_query(\"Who won the IIOTY award?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909a1ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question: str, history: list[dict] = []) -> tuple[str, list]:\n",
    "    \"\"\"\n",
    "    Answer a question using RAG and return the answer and the retrieved context\n",
    "    \"\"\"\n",
    "    query = rewrite_query(question, history)\n",
    "    print(query)\n",
    "    chunks = fetch_context(query)\n",
    "    messages = make_rag_messages(question, history, chunks)\n",
    "    response = completion(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content, chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ab3e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Who won the IIOTY award?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f943a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3237722e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_question(\"Who went to Manchester University?\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c93b30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
